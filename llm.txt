from typing import Dict, Any, Optional
import os

# Default models
DEFAULT_CHAT_MODEL = "openrouter/google/gemini-2.0-flash-exp:free"
DEFAULT_INFERENCE_MODEL = "openrouter/google/gemini-2.0-flash-exp:free"

# Model configuration mappings - now keyed by full model name
MODEL_CONFIG_MAP = {
    # OpenAI Models
    "openai/gpt-4.1-mini": {
        "provider": "openai",
        "default_params": {"temperature": 0.3},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    "openai/gpt-4.1": {
        "provider": "openai",
        "default_params": {"temperature": 0.3},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    "openai/gpt-4o": {
        "provider": "openai",
        "default_params": {"temperature": 0.3},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    # Anthropic Models
    "anthropic/claude-haiku-4-5-20251001": {
        "provider": "anthropic",
        "default_params": {"temperature": 0.2, "max_tokens": 8000},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    "anthropic/claude-sonnet-4-5-20250929": {
        "provider": "anthropic",
        "default_params": {"temperature": 0.3, "max_tokens": 8000},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    "anthropic/claude-sonnet-4-20250514": {
        "provider": "anthropic",
        "default_params": {"temperature": 0.3, "max_tokens": 8000},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    "anthropic/claude-opus-4-1-20250805": {
        "provider": "anthropic",
        "default_params": {"temperature": 0.3, "max_tokens": 8000},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    "anthropic/claude-3-7-sonnet-20250219": {
        "provider": "anthropic",
        "default_params": {"temperature": 0.3, "max_tokens": 8000},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    "anthropic/claude-3-5-haiku-20241022": {
        "provider": "anthropic",
        "default_params": {"temperature": 0.2, "max_tokens": 8000},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": None,
        "api_version": None,
    },
    # DeepSeek Models
    "openrouter/deepseek/deepseek-chat-v3-0324": {
        "provider": "deepseek",
        "auth_provider": "openrouter",
        "default_params": {"temperature": 0.3, "max_tokens": 8000},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": False,
            "supports_tool_parallelism": True,
        },
        "base_url": "https://openrouter.ai/api/v1",
        "api_version": None,
    },
    # Meta-Llama Models
    "openrouter/meta-llama/llama-3.3-70b-instruct": {
        "provider": "meta-llama",
        "auth_provider": "openrouter",
        "default_params": {"temperature": 0.3},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": False,
            "supports_tool_parallelism": True,
        },
        "base_url": "https://openrouter.ai/api/v1",
        "api_version": None,
    },
    # Gemini Models
    "openrouter/google/gemini-2.0-flash-001": {
        "provider": "gemini",
        "auth_provider": "openrouter",
        "default_params": {"temperature": 0.3},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": "https://openrouter.ai/api/v1",
        "api_version": None,
    },
    "openrouter/google/gemini-2.5-pro-preview": {
        "provider": "gemini",
        "auth_provider": "openrouter",
        "default_params": {"temperature": 0.3},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": "https://openrouter.ai/api/v1",
        "api_version": None,
    },
    "openrouter/google/gemini-2.0-flash-exp:free": {
        "provider": "gemini",
        "auth_provider": "openrouter",
        "default_params": {"temperature": 0.3},
        "capabilities": {
            "supports_pydantic": True,
            "supports_streaming": True,
            "supports_vision": True,
            "supports_tool_parallelism": True,
        },
        "base_url": "https://openrouter.ai/api/v1",
        "api_version": None,
    },
}


class LLMProviderConfig:
    def __init__(
        self,
        provider: str,
        model: str,
        default_params: Dict[str, Any],
        capabilities: Dict[str, bool],
        base_url: Optional[str] = None,
        api_version: Optional[str] = None,
        auth_provider: Optional[str] = None,
    ):
        self.provider = provider
        self.auth_provider = auth_provider or provider
        self.model = model
        self.default_params = default_params
        self.capabilities = dict(capabilities) if capabilities else {}

        env_base_url = os.environ.get("LLM_API_BASE")
        env_api_version = os.environ.get("LLM_API_VERSION")

        self.base_url = base_url or env_base_url
        self.api_version = api_version or env_api_version

        capability_overrides = {
            "supports_pydantic": _normalize_bool_env("LLM_SUPPORTS_PYDANTIC"),
            "supports_streaming": _normalize_bool_env("LLM_SUPPORTS_STREAMING"),
            "supports_vision": _normalize_bool_env("LLM_SUPPORTS_VISION"),
            "supports_tool_parallelism": _normalize_bool_env(
                "LLM_SUPPORTS_TOOL_PARALLELISM"
            ),
        }
        for key, override in capability_overrides.items():
            if override is not None:
                self.capabilities[key] = override

    def get_llm_params(self, api_key: str) -> Dict[str, Any]:
        """Build a complete parameter dictionary for LLM calls."""
        params = {
            "model": self.model,
            "temperature": self.default_params.get("temperature", 0.3),
            "api_key": api_key,
        }
        # Add any additional default parameters
        for key, value in self.default_params.items():
            if key != "temperature":  # temperature already handled above
                params[key] = value
        return params


def _normalize_bool_env(var_name: str) -> Optional[bool]:
    """Return a boolean override from the environment if present."""
    raw_value = os.environ.get(var_name)
    if raw_value is None:
        return None
    return raw_value.strip().lower() in {"1", "true", "yes", "on"}


def parse_model_string(model_string: str) -> tuple[str, str]:
    """Parse a model string into provider and model name."""
    try:
        parts = model_string.split("/")
        provider = parts[0]

        if provider == "ollama_chat":
            provider = "ollama"
            full_model_name = "ollama/" + "/".join(parts[1:])
        else:
            full_model_name = model_string

        return provider, full_model_name
    except (IndexError, AttributeError):
        return "openai", DEFAULT_CHAT_MODEL


def get_config_for_model(model_string: str) -> Dict[str, Any]:
    """Get configuration for a specific model, with fallback to defaults."""
    if model_string in MODEL_CONFIG_MAP:
        return MODEL_CONFIG_MAP[model_string]
    # If model not found, use default configuration based on provider
    provider, _ = parse_model_string(model_string)
    env_base_url = os.environ.get("LLM_API_BASE")
    supports_pydantic = provider in {
        "openai",
        "anthropic",
        "openrouter",
        "azure",
        "ollama",
    }
    return {
        "provider": provider,
        "default_params": {"temperature": 0.3},
        "capabilities": {
            "supports_pydantic": supports_pydantic or bool(env_base_url),
            "supports_streaming": True,
            "supports_vision": provider in {"openai", "anthropic"},
            "supports_tool_parallelism": provider in {"openai", "anthropic"},
        },
        "base_url": None,
        "api_version": None,
        "auth_provider": provider,
    }


def build_llm_provider_config(
    user_pref: dict, config_type: str = "chat"
) -> LLMProviderConfig:
    """
    Build an LLMProviderConfig based on the environment variables, user preferences, and defaults.
    Config type can be 'chat' or 'inference'.

    Priority order:
    1. Environment variables (CHAT_MODEL or INFERENCE_MODEL)
    2. User preferences (chat_model or inference_model)
    3. Built-in defaults
    """
    # Determine which model to use based on config_type and priority order
    if config_type == "chat":
        model_string = (
            os.environ.get("CHAT_MODEL")
            or user_pref.get("chat_model")
            or DEFAULT_CHAT_MODEL
        )
    else:
        model_string = (
            os.environ.get("INFERENCE_MODEL")
            or user_pref.get("inference_model")
            or DEFAULT_INFERENCE_MODEL
        )

    # Get provider and configuration for the model
    provider, full_model_name = parse_model_string(model_string)
    config_data = get_config_for_model(full_model_name).copy()

    return LLMProviderConfig(
        provider=config_data["provider"],
        model=full_model_name,
        default_params=dict(config_data["default_params"]),
        capabilities=config_data.get("capabilities", {}),
        base_url=config_data.get("base_url"),
        api_version=config_data.get("api_version"),
        auth_provider=config_data.get("auth_provider"),
    )









    from typing import List

from fastapi import HTTPException
from sqlalchemy.orm import Session

from .provider_schema import (
    GetProviderResponse,
    ProviderInfo,
    SetProviderRequest,
    AvailableModelsResponse,
)
from .provider_service import ProviderService


class ProviderController:
    def __init__(self, db: Session, user_id: str):
        self.service = ProviderService.create(db, user_id)
        self.user_id = user_id

    async def list_available_llms(self) -> List[ProviderInfo]:
        """List available LLM providers."""
        try:
            providers = await self.service.list_available_llms()
            return providers
        except Exception as e:
            raise HTTPException(
                status_code=500, detail=f"Error listing LLM providers: {str(e)}"
            )

    async def list_available_models(self) -> AvailableModelsResponse:
        """List available models for both chat and inference."""
        try:
            models = await self.service.list_available_models()
            return models
        except Exception as e:
            raise HTTPException(
                status_code=500, detail=f"Error listing available models: {str(e)}"
            )

    async def set_global_ai_provider(self, user_id: str, request: SetProviderRequest):
        """Update the global AI provider configuration."""
        try:
            result = await self.service.set_global_ai_provider(user_id, request)
            return result
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Error setting global AI provider: {str(e)}",
            )

    async def get_global_ai_provider(self, user_id: str) -> GetProviderResponse:
        """Get the current global AI provider configuration."""
        try:
            return await self.service.get_global_ai_provider(user_id)
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Error getting global AI provider: {str(e)}",
            )


import logging
import os
from typing import List, Dict, Any, Union, AsyncGenerator, Optional
import json
from pydantic import BaseModel
try:
    from pydantic_ai.models import Model
except Exception:
    class _AnyModel:
        pass
    Model = _AnyModel
try:
    from litellm import litellm, AsyncOpenAI, acompletion
except Exception:
    class _LiteLLMStub:
        num_retries = 0
    litellm = _LiteLLMStub()
    AsyncOpenAI = None
    async def acompletion(*args, **kwargs):
        raise RuntimeError("litellm not available")
try:
    import instructor
except Exception:
    instructor = None

class _ConfigProvider:
    def get_is_multimodal_enabled(self) -> bool:
        val = os.environ.get("LLM_SUPPORTS_VISION")
        if val is None:
            return False
        return val.strip().lower() in {"1", "true", "yes", "on"}
config_provider = _ConfigProvider()

logger = logging.getLogger(__name__)

from .provider_schema import (
    ProviderInfo,
    GetProviderResponse,
    AvailableModelsResponse,
    AvailableModelOption,
    SetProviderRequest,
    ModelInfo,
)
from .llm_config import (
    LLMProviderConfig,
    build_llm_provider_config,
    get_config_for_model,
)
from .exceptions import UnsupportedProviderError

try:
    from pydantic_ai.models.openai import OpenAIModel
    from pydantic_ai.models.anthropic import AnthropicModel
    from pydantic_ai.providers.openai import OpenAIProvider
    from pydantic_ai.providers.anthropic import AnthropicProvider
except Exception:
    OpenAIModel = AnthropicModel = OpenAIProvider = AnthropicProvider = None
try:
    import litellm as _litellm_mod
    litellm = _litellm_mod
except Exception:
    pass

import random
import time
import asyncio
from functools import wraps

litellm.num_retries = 5  # Number of retries for rate limited requests

OVERLOAD_ERROR_PATTERNS = {
    "anthropic": ["overloaded", "overloaded_error", "capacity", "rate limit exceeded"],
    "openai": [
        "rate_limit_exceeded",
        "capacity",
        "overloaded",
        "server_error",
        "timeout",
    ],
    "general": [
        "timeout",
        "insufficient capacity",
        "server_error",
        "internal_server_error",
    ],
}


class RetrySettings:
    """Configuration class for retry behavior"""

    def __init__(
        self,
        max_retries: int = 8,
        min_delay: float = 1.0,
        max_delay: float = 120.0,
        base_delay: float = 2.0,
        jitter_factor: float = 0.2,
        step_increase: float = 1.8,
        # Set what types of errors should be retried
        retry_on_timeout: bool = True,
        retry_on_overloaded: bool = True,
        retry_on_rate_limit: bool = True,
        retry_on_server_error: bool = True,
    ):
        self.max_retries = max_retries
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.base_delay = base_delay
        self.jitter_factor = jitter_factor
        self.step_increase = step_increase
        self.retry_on_timeout = retry_on_timeout
        self.retry_on_overloaded = retry_on_overloaded
        self.retry_on_rate_limit = retry_on_rate_limit
        self.retry_on_server_error = retry_on_server_error


def identify_provider_from_error(error: Exception) -> str:
    """Identify the provider from an exception"""
    error_str = str(error).lower()

    # Try to identify provider from error message
    for provider in ["anthropic", "openai", "cohere", "azure"]:
        if provider.lower() in error_str.lower():
            return provider

    return "unknown"


def is_recoverable_error(error: Exception, settings: RetrySettings) -> bool:
    """Determine if an error is recoverable based on retry settings"""
    error_str = str(error).lower()
    provider = identify_provider_from_error(error)

    # Check for timeout errors
    if settings.retry_on_timeout and "timeout" in error_str:
        return True

    # Check for overloaded errors
    if settings.retry_on_overloaded:
        overload_patterns = (
            OVERLOAD_ERROR_PATTERNS.get(provider, [])
            + OVERLOAD_ERROR_PATTERNS["general"]
        )
        if any(pattern in error_str for pattern in overload_patterns):
            return True

    # Check for rate limit errors
    if settings.retry_on_rate_limit and any(
        limit_pattern in error_str
        for limit_pattern in [
            "rate limit",
            "rate_limit",
            "ratelimit",
            "requests per minute",
        ]
    ):
        return True

    # Check for server errors
    if settings.retry_on_server_error and any(
        server_err in error_str
        for server_err in [
            "server_error",
            "internal_server_error",
            "500",
            "502",
            "503",
            "504",
        ]
    ):
        return True

    return False


def calculate_backoff_time(retry_count: int, settings: RetrySettings) -> float:
    """Calculate exponential backoff with jitter"""
    # Calculate base exponential backoff
    delay = min(
        settings.max_delay, settings.base_delay * (settings.step_increase**retry_count)
    )

    # Add jitter to avoid thundering herd problem
    jitter = random.uniform(1 - settings.jitter_factor, 1 + settings.jitter_factor)

    # Ensure we stay within our bounds
    final_delay = max(settings.min_delay, min(settings.max_delay, delay * jitter))

    return final_delay


# Create a custom retry function for litellm
def custom_litellm_retry_handler(retry_count: int, exception: Exception) -> bool:
    """
    Custom retry handler for litellm's built-in retry mechanism
    This gets registered with litellm.custom_retry_fn
    """
    # Default settings for litellm's built-in retry
    settings = RetrySettings(max_retries=litellm.num_retries)

    if not is_recoverable_error(exception, settings):
        # If it's not a recoverable error, don't retry
        return False

    delay = calculate_backoff_time(retry_count, settings)

    provider = identify_provider_from_error(exception)
    logging.warning(
        f"{provider.capitalize()} API error: {str(exception)}. "
        f"Retry {retry_count}/{settings.max_retries}, "
        f"waiting {delay:.2f}s before next attempt..."
    )

    time.sleep(delay)
    return True


# Decorator for robust LLM calls with advanced error handling
def robust_llm_call(settings: Optional[RetrySettings] = None):
    """
    Decorator for robust handling of LLM API calls with exponential backoff
    """
    if settings is None:
        settings = RetrySettings()

    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            retries = 0
            last_exception = None

            while retries <= settings.max_retries:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e

                    if not is_recoverable_error(e, settings):
                        # If it's not a recoverable error, just raise
                        raise

                    provider = identify_provider_from_error(e)

                    if retries >= settings.max_retries:
                        logging.error(
                            f"Max retries ({settings.max_retries}) exceeded for {provider} API call. "
                            f"Last error: {str(e)}"
                        )
                        raise

                    delay = calculate_backoff_time(retries, settings)

                    logging.warning(
                        f"{provider.capitalize()} API error: {str(e)}. "
                        f"Retry {retries+1}/{settings.max_retries}, "
                        f"waiting {delay:.2f}s before next attempt..."
                    )

                    await asyncio.sleep(delay)
                    retries += 1

            # This should never be reached due to the raise in the loop,
            # but included for clarity
            raise last_exception

        return wrapper

    return decorator


# Available models with their metadata
AVAILABLE_MODELS = [
    AvailableModelOption(
        id="openai/gpt-4.1",
        name="GPT-4.1",
        description="OpenAI's latest model for complex tasks with large context",
        provider="openai",
        is_chat_model=True,
        is_inference_model=False,
    ),
    AvailableModelOption(
        id="openai/gpt-4o",
        name="GPT-4o",
        description="High-intelligence model for complex tasks",
        provider="openai",
        is_chat_model=True,
        is_inference_model=False,
    ),
    AvailableModelOption(
        id="openai/gpt-4.1-mini",
        name="GPT-4.1 Mini",
        description="Smaller model for fast, lightweight tasks",
        provider="openai",
        is_chat_model=False,
        is_inference_model=True,
    ),
    AvailableModelOption(
        id="openai/o4-mini",
        name="O4 mini",
        description="reasoning model",
        provider="openai",
        is_chat_model=True,
        is_inference_model=True,
    ),
    AvailableModelOption(
        id="anthropic/claude-sonnet-4-5-20250929",
        name="Claude Sonnet 4.5",
        description="Best model for complex agents and coding",
        provider="anthropic",
        is_chat_model=True,
        is_inference_model=False,
    ),
    AvailableModelOption(
        id="anthropic/claude-haiku-4-5-20251001",
        name="Claude Haiku 4.5",
        description="Faster, even surpasses Claude Sonnet 4 at certain tasks",
        provider="anthropic",
        is_chat_model=True,
        is_inference_model=True,
    ),
    AvailableModelOption(
        id="anthropic/claude-opus-4-1-20250805",
        name="Claude Opus 4.1",
        description="Exceptional model for specialized complex tasks",
        provider="anthropic",
        is_chat_model=True,
        is_inference_model=False,
    ),
    AvailableModelOption(
        id="anthropic/claude-sonnet-4-20250514",
        name="Claude Sonnet 4",
        description="Faster, more efficient Claude model for code generation",
        provider="anthropic",
        is_chat_model=True,
        is_inference_model=False,
    ),
    AvailableModelOption(
        id="anthropic/claude-3-7-sonnet-20250219",
        name="Claude Sonnet 3.7",
        description="Highest level of intelligence and capability with toggleable extended thinking",
        provider="anthropic",
        is_chat_model=True,
        is_inference_model=False,
    ),
    AvailableModelOption(
        id="anthropic/claude-3-5-haiku-20241022",
        name="Claude Haiku 3.5",
        description="Faster, more efficient Claude model",
        provider="anthropic",
        is_chat_model=False,
        is_inference_model=True,
    ),
    AvailableModelOption(
        id="openrouter/deepseek/deepseek-chat-v3-0324",
        name="DeepSeek V3",
        description="DeepSeek's latest chat model",
        provider="deepseek",
        is_chat_model=True,
        is_inference_model=True,
    ),
    AvailableModelOption(
        id="openrouter/meta-llama/llama-3.3-70b-instruct",
        name="Llama 3.3 70B",
        description="Meta's latest Llama model",
        provider="meta-llama",
        is_chat_model=True,
        is_inference_model=True,
    ),
    AvailableModelOption(
        id="openrouter/google/gemini-2.0-flash-001",
        name="Gemini 2.0 Flash",
        description="Google's Gemini model optimized for speed",
        provider="gemini",
        is_chat_model=True,
        is_inference_model=True,
    ),
    AvailableModelOption(
        id="openrouter/google/gemini-2.5-pro-preview",
        name="Gemini 2.5 Pro",
        description="Google's Latest pro Gemini model",
        provider="gemini",
        is_chat_model=True,
        is_inference_model=True,
    ),
]

# Extract unique platform providers from the available models
PLATFORM_PROVIDERS = list(
    {model.provider for model in AVAILABLE_MODELS}
    | {
        get_config_for_model(model.id).get("auth_provider", model.provider)
        for model in AVAILABLE_MODELS
    }
)


class ProviderService:
    def __init__(self, db, user_id: str):
        try:
            litellm.modify_params = True
        except Exception:
            pass
        self.db = db
        self.user_id = user_id

        user_config = {}
        self.chat_config = build_llm_provider_config(user_config, config_type="chat")
        self.inference_config = build_llm_provider_config(
            user_config, config_type="inference"
        )

        self.retry_settings = RetrySettings(
            max_retries=8, base_delay=2.0, max_delay=120.0
        )

    @classmethod
    def create(cls, db, user_id: str):
        return cls(db, user_id)

    async def list_available_llms(self) -> List[ProviderInfo]:
        # Get unique providers from available models
        providers = {
            model.provider: ProviderInfo(
                id=model.provider,
                name=model.provider,
                description=f"Provider for {model.provider} models",
            )
            for model in AVAILABLE_MODELS
        }
        return list(providers.values())

    async def list_available_models(self) -> AvailableModelsResponse:
        return AvailableModelsResponse(models=AVAILABLE_MODELS)

    async def set_global_ai_provider(self, user_id: str, request: SetProviderRequest):
        if request.chat_model:
            os.environ["CHAT_MODEL"] = request.chat_model
            self.chat_config = build_llm_provider_config({"chat_model": request.chat_model}, "chat")
        if request.inference_model:
            os.environ["INFERENCE_MODEL"] = request.inference_model
            self.inference_config = build_llm_provider_config({"inference_model": request.inference_model}, "inference")
        return {"message": "AI provider configuration updated successfully"}

    def _get_api_key(self, provider: str) -> str:
        """Get API key for the specified provider."""
        env_key = os.getenv("LLM_API_KEY", None)
        if env_key:
            return env_key

        env_key = os.getenv(f"{provider.upper()}_API_KEY")
        if env_key:
            return env_key
        return None

    def _build_llm_params(self, config: LLMProviderConfig) -> Dict[str, Any]:
        """Build a dictionary of parameters for LLM initialization."""
        api_key = self._get_api_key(config.auth_provider)
        if not api_key and config.auth_provider == "ollama":
            api_key = os.environ.get("OLLAMA_API_KEY", "ollama")
        if not api_key:
            api_key = os.environ.get("LLM_API_KEY", api_key)

        params = config.get_llm_params(api_key)

        if config.base_url:
            base_url = config.base_url
            if config.auth_provider == "ollama":
                base_url = base_url.rstrip("/")
                if base_url.endswith("/v1"):
                    base_url = base_url[:-3]
            params["base_url"] = base_url
        elif config.auth_provider == "ollama":
            params["base_url"] = os.environ.get(
                "LLM_API_BASE", "http://localhost:11434"
            )
        if config.api_version:
            params["api_version"] = config.api_version

        if config.auth_provider == "openrouter":
            referer = os.environ.get("OPENROUTER_SITE_URL")
            title = os.environ.get("OPENROUTER_APP_TITLE")
            if not referer or not title:
                try:
                    from core.config import settings
                    referer = referer or settings.BASE_URL
                    title = title or settings.APP_NAME
                except Exception:
                    referer = referer or "http://localhost:8000"
                    title = title or "Email Classification API"
            extra_headers = {
                "HTTP-Referer": referer,
                "X-Title": title,
                "Content-Type": "application/json",
            }
            if api_key:
                extra_headers["Authorization"] = f"Bearer {api_key}"
            params["extra_headers"] = extra_headers

        # Filter out falsy values litellm would not expect
        return {key: value for key, value in params.items() if value is not None}

    def _build_config_for_model_identifier(
        self, model_identifier: str
    ) -> LLMProviderConfig:
        """Create a provider config for a specific model identifier."""
        config_data = get_config_for_model(model_identifier).copy()
        default_params = dict(config_data.get("default_params", {}))

        return LLMProviderConfig(
            provider=config_data["provider"],
            model=model_identifier,
            default_params=default_params,
            capabilities=config_data.get("capabilities", {}),
            base_url=config_data.get("base_url"),
            api_version=config_data.get("api_version"),
            auth_provider=config_data.get("auth_provider"),
        )

    async def get_global_ai_provider(self, user_id: str) -> GetProviderResponse:
        """Get the current global AI provider configuration."""
        try:
            chat_model_id = os.environ.get("CHAT_MODEL") or "openai/gpt-4o"
            inference_model_id = os.environ.get("INFERENCE_MODEL") or "openai/gpt-4.1-mini"

            # Default values
            chat_provider = chat_model_id.split("/")[0] if chat_model_id else ""
            chat_model_name = chat_model_id

            inference_provider = (
                inference_model_id.split("/")[0] if inference_model_id else ""
            )
            inference_model_name = inference_model_id

            # Find matching model in AVAILABLE_MODELS to get proper names
            for model in AVAILABLE_MODELS:
                if model.id == chat_model_id:
                    chat_model_name = model.name
                    chat_provider = model.provider

                if model.id == inference_model_id:
                    inference_model_name = model.name
                    inference_provider = model.provider

            # Create response with nested ModelInfo objects
            return GetProviderResponse(
                chat_model=ModelInfo(
                    provider=chat_provider, id=chat_model_id, name=chat_model_name
                ),
                inference_model=ModelInfo(
                    provider=inference_provider,
                    id=inference_model_id,
                    name=inference_model_name,
                ),
            )
        except Exception as e:
            logging.error(f"Error getting global AI provider: {e}")
            raise e

    def supports_pydantic(self, config_type: str = "chat") -> bool:
        """Return True when the active model supports the pydantic-ai stack."""
        config = self.chat_config if config_type == "chat" else self.inference_config
        return config.capabilities.get("supports_pydantic", False)

    @robust_llm_call()
    async def call_llm_with_specific_model(
        self,
        model_identifier: str,
        messages: list,
        output_schema: Optional[BaseModel] = None,
        stream: bool = False,
        **kwargs,
    ) -> Union[str, AsyncGenerator[str, None], Any]:
        """Call LLM with a specific model identifier (e.g., 'openrouter/perplexity/sonar')."""
        # Build configuration for the specific model
        config = self._build_config_for_model_identifier(model_identifier)
        print("config",config)
        # Build parameters using the config object
        params = self._build_llm_params(config)

        # Override with any additional parameters
        params.update(kwargs)

        routing_provider = config.provider

        try:
            if output_schema:
                # Use structured output with instructor
                request_kwargs = {
                    key: params[key]
                    for key in ("api_key", "base_url", "api_version")
                    if key in params
                }

                if config.provider == "ollama":
                    # use openai client to call ollama because of https://github.com/BerriAI/litellm/issues/7355
                    ollama_base_root = (
                        params.get("base_url")
                        or config.base_url
                        or os.environ.get("LLM_API_BASE")
                        or "http://localhost:11434"
                    )
                    ollama_base_url = ollama_base_root.rstrip("/") + "/v1"
                    ollama_api_key = params.get("api_key") or os.environ.get(
                        "OLLAMA_API_KEY", "ollama"
                    )
                    client = instructor.from_openai(
                        AsyncOpenAI(base_url=ollama_base_url, api_key=ollama_api_key),
                        mode=instructor.Mode.JSON,
                    )
                    ollama_request_kwargs = {
                        key: value
                        for key, value in request_kwargs.items()
                        if key not in {"base_url", "api_key", "api_version"}
                    }
                    response = await client.chat.completions.create(
                        model=params["model"].split("/")[-1],
                        messages=messages,
                        response_model=output_schema,
                        temperature=params.get("temperature", 0.3),
                        max_tokens=params.get("max_tokens"),
                        **ollama_request_kwargs,
                    )
                else:
                    client = instructor.from_litellm(
                        acompletion, mode=instructor.Mode.JSON
                    )
                    response = await client.chat.completions.create(
                        model=params["model"],
                        messages=messages,
                        response_model=output_schema,
                        strict=True,
                        temperature=params.get("temperature", 0.3),
                        max_tokens=params.get("max_tokens"),
                        **request_kwargs,
                    )
                return response
            else:
                # Regular text completion
                if stream:

                    async def generator() -> AsyncGenerator[str, None]:
                        response = await acompletion(
                            messages=messages, stream=True, **params
                        )
                        async for chunk in response:
                            yield chunk.choices[0].delta.content or ""

                    return generator()
                else:
                    response = await acompletion(messages=messages, **params)
                    return response.choices[0].message.content
        except Exception as e:
            logging.error(
                f"Error calling LLM with model {model_identifier}: {e}, provider: {routing_provider}"
            )
            raise e

    @robust_llm_call()  # Apply the robust_llm_call decorator
    async def call_llm(
        self, messages: list, stream: bool = False, config_type: str = "chat"
    ) -> Union[str, AsyncGenerator[str, None]]:
        """Call LLM with the specified messages with robust error handling."""
        # Select the appropriate config based on config_type
        config = self.chat_config if config_type == "chat" else self.inference_config

        # Build parameters using the config object
        params = self._build_llm_params(config)
        routing_provider = config.provider

        # Handle streaming response if requested
        try:
            if stream:

                async def generator() -> AsyncGenerator[str, None]:
                    response = await acompletion(
                        messages=messages, stream=True, **params
                    )
                    async for chunk in response:
                        yield chunk.choices[0].delta.content or ""

                return generator()
            else:
                response = await acompletion(messages=messages, **params)
                return response.choices[0].message.content
        except Exception as e:
            logging.error(f"Error calling LLM: {e}, provider: {routing_provider}")
            raise e

    @robust_llm_call()
    async def call_llm_with_structured_output(
        self, messages: list, output_schema: BaseModel, config_type: str = "chat"
    ) -> Any:
        """Call LLM and parse the response into a structured output using a Pydantic model."""
        # Select the appropriate config
        config = self.chat_config if config_type == "chat" else self.inference_config

        # Build parameters
        params = self._build_llm_params(config)
        routing_provider = config.provider

        request_kwargs = {
            key: params[key]
            for key in ("api_key", "base_url", "api_version")
            if key in params
        }

        try:
            if instructor is not None:
                if config.provider == "ollama":
                    ollama_base_root = (
                        params.get("base_url")
                        or config.base_url
                        or os.environ.get("LLM_API_BASE")
                        or "http://localhost:11434"
                    )
                    ollama_base_url = ollama_base_root.rstrip("/") + "/v1"
                    ollama_api_key = params.get("api_key") or os.environ.get(
                        "OLLAMA_API_KEY", "ollama"
                    )
                    client = instructor.from_openai(
                        AsyncOpenAI(base_url=ollama_base_url, api_key=ollama_api_key),
                        mode=instructor.Mode.JSON,
                    )
                    ollama_request_kwargs = {
                        key: value
                        for key, value in request_kwargs.items()
                        if key not in {"base_url", "api_key", "api_version"}
                    }
                    response = await client.chat.completions.create(
                        model=params["model"].split("/")[-1],
                        messages=messages,
                        response_model=output_schema,
                        temperature=params.get("temperature", 0.3),
                        max_tokens=params.get("max_tokens"),
                        **ollama_request_kwargs,
                    )
                else:
                    client = instructor.from_litellm(acompletion, mode=instructor.Mode.JSON)
                    response = await client.chat.completions.create(
                        model=params["model"],
                        messages=messages,
                        response_model=output_schema,
                        strict=True,
                        temperature=params.get("temperature", 0.3),
                        max_tokens=params.get("max_tokens"),
                        **request_kwargs,
                    )
                return response
            else:
                fields = []
                try:
                    if hasattr(output_schema, "model_fields"):
                        fields = list(output_schema.model_fields.keys())
                    elif hasattr(output_schema, "__fields__"):
                        fields = list(output_schema.__fields__.keys())
                except Exception:
                    fields = []
                extra = {
                    "role": "system",
                    "content": "Return a JSON object with keys: "
                    + ", ".join(fields or ["category", "confidence", "reason"]) + ".",
                }
                m = messages + [extra]
                resp = await acompletion(messages=m, **params)
                content = resp.choices[0].message.content
                data = {}
                try:
                    data = json.loads(content)
                except Exception:
                    for ln in content.splitlines():
                        if ":" in ln:
                            k, v = ln.split(":", 1)
                            data[k.strip().lower()] = v.strip()
                    if "category" not in data:
                        data["category"] = "OTHERS"
                try:
                    mapped = {}
                    for k in (fields or ["category", "confidence", "reason"]):
                        mapped[k] = data.get(k) or data.get(k.lower())
                    return output_schema(**mapped)
                except Exception:
                    return output_schema(**{"category": "OTHERS", "confidence": None, "reason": None})
        except Exception as e:
            logging.error(f"LLM call with structured output failed: {e}")
            return output_schema(**{"category": "OTHERS", "confidence": None, "reason": None})

    @robust_llm_call()
    async def call_llm_multimodal(
        self,
        messages: List[Dict[str, Any]],
        images: Optional[Dict[str, Dict[str, Union[str, int]]]] = None,
        stream: bool = False,
        config_type: str = "chat",
    ) -> Union[str, AsyncGenerator[str, None]]:
        """Call LLM with multimodal support (text + images)"""

        # Check if multimodal is enabled
        if not config_provider.get_is_multimodal_enabled():
            logger.info("Multimodal disabled - falling back to text-only processing")
            return await self.call_llm(messages, stream=stream, config_type=config_type)

        # If no images provided, use standard text-only call
        if not images:
            return await self.call_llm(messages, stream=stream, config_type=config_type)

        # Original multimodal logic continues...
        # Select the appropriate config based on config_type
        config = self.chat_config if config_type == "chat" else self.inference_config

        # Build parameters using the config object
        params = self._build_llm_params(config)
        routing_provider = config.provider

        # Validate and filter images before processing
        if images:
            validated_images = self._validate_images_for_multimodal(images)
            if validated_images:
                messages = self._format_multimodal_messages(
                    messages, validated_images, routing_provider
                )
                logger.info(
                    f"Using {len(validated_images)} validated images out of {len(images)} provided for provider {routing_provider}"
                )
            else:
                logger.warning(
                    "No valid images after validation, proceeding with text-only"
                )
                images = None

        # Handle streaming response if requested
        try:
            if stream:

                async def generator() -> AsyncGenerator[str, None]:
                    response = await acompletion(
                        messages=messages, stream=True, **params
                    )
                    async for chunk in response:
                        yield chunk.choices[0].delta.content or ""

                return generator()
            else:
                response = await acompletion(messages=messages, **params)
                return response.choices[0].message.content
        except Exception as e:
            logging.error(
                f"Error calling multimodal LLM: {e}, provider: {routing_provider}"
            )
            raise e

    def _format_multimodal_messages(
        self,
        messages: List[Dict[str, Any]],
        images: Dict[str, Dict[str, Union[str, int]]],
        provider: str,
    ) -> List[Dict[str, Any]]:
        """Format messages for provider-specific multimodal format"""
        if not images:
            return messages

        formatted_messages = []

        for message in messages:
            if (
                message.get("role") == "user"
                and len(formatted_messages) == len(messages) - 1
            ):
                # This is the last user message - add images to it
                formatted_message = self._format_multimodal_message(
                    message, images, provider
                )
                formatted_messages.append(formatted_message)
            else:
                formatted_messages.append(message)

        return formatted_messages

    def _format_multimodal_message(
        self,
        message: Dict[str, Any],
        images: Dict[str, Dict[str, Union[str, int]]],
        provider: str,
    ) -> Dict[str, Any]:
        """Format a single message for provider-specific multimodal format"""
        text_content = message.get("content", "")

        if provider == "openai":
            return self._format_openai_multimodal_message(text_content, images)
        elif provider == "anthropic":
            return self._format_anthropic_multimodal_message(text_content, images)
        elif provider == "gemini":
            return self._format_gemini_multimodal_message(text_content, images)
        else:
            # Fallback to OpenAI format for unknown providers
            logger.warning(
                f"Unknown provider {provider}, using OpenAI format for multimodal"
            )
            return self._format_openai_multimodal_message(text_content, images)

    def _format_openai_multimodal_message(
        self, text: str, images: Dict[str, Dict[str, Union[str, int]]]
    ) -> Dict[str, Any]:
        """Format message for OpenAI GPT-4V format"""
        content = [{"type": "text", "text": text}]

        for attachment_id, image_data in images.items():
            mime_type = image_data.get("mime_type", "image/jpeg")
            base64_data = image_data["base64"]

            content.append(
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{mime_type};base64,{base64_data}",
                        "detail": "high",  # Use high detail for better analysis
                    },
                }
            )

        return {"role": "user", "content": content}

    def _format_anthropic_multimodal_message(
        self, text: str, images: Dict[str, Dict[str, Union[str, int]]]
    ) -> Dict[str, Any]:
        """Format message for Anthropic Claude Vision format"""
        content = []

        # Add images first for Claude
        for attachment_id, image_data in images.items():
            mime_type = image_data.get("mime_type", "image/jpeg")
            base64_data = image_data["base64"]

            content.append(
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": mime_type,
                        "data": base64_data,
                    },
                }
            )

        # Add text content
        content.append({"type": "text", "text": text})

        return {"role": "user", "content": content}

    def _format_gemini_multimodal_message(
        self, text: str, images: Dict[str, Dict[str, Union[str, int]]]
    ) -> Dict[str, Any]:
        """Format message for Google Gemini Vision format (uses OpenAI-compatible format via OpenRouter)"""
        return self._format_openai_multimodal_message(text, images)

    def _validate_images_for_multimodal(
        self, images: Dict[str, Dict[str, Union[str, int]]]
    ) -> Dict[str, Dict[str, Union[str, int]]]:
        """Validate images before sending to multimodal LLM to reduce hallucinations"""
        validated_images = {}

        for img_id, img_data in images.items():
            try:
                # Check required fields
                if "base64" not in img_data or not img_data["base64"]:
                    logger.warning(
                        f"Skipping image {img_id}: missing or empty base64 data"
                    )
                    continue

                base64_data = str(img_data["base64"])

                # Check base64 data length (reasonable bounds)
                if len(base64_data) < 100:  # Too small to be a valid image
                    logger.warning(
                        f"Skipping image {img_id}: base64 data too small ({len(base64_data)} chars)"
                    )
                    continue

                if (
                    len(base64_data) > 10_000_000
                ):  # Over ~7MB base64 (too large for most APIs)
                    logger.warning(
                        f"Skipping image {img_id}: base64 data too large ({len(base64_data)} chars)"
                    )
                    continue

                # Check MIME type
                mime_type = img_data.get("mime_type", "")
                supported_types = ["image/jpeg", "image/png", "image/webp", "image/gif"]
                if mime_type not in supported_types:
                    logger.warning(
                        f"Skipping image {img_id}: unsupported MIME type {mime_type}"
                    )
                    continue

                # Basic base64 validation (should start with valid characters)
                if (
                    not base64_data.replace("+", "")
                    .replace("/", "")
                    .replace("=", "")
                    .isalnum()
                ):
                    logger.warning(f"Skipping image {img_id}: invalid base64 encoding")
                    continue

                # Image passed validation
                validated_images[img_id] = img_data
                logger.debug(
                    f"Image {img_id} passed validation ({len(base64_data)} chars, {mime_type})"
                )

            except Exception as e:
                logger.error(f"Error validating image {img_id}: {str(e)}")
                continue

        logger.info(
            f"Validated {len(validated_images)} out of {len(images)} images for multimodal processing"
        )
        return validated_images

    def is_vision_model(self, config_type: str = "chat") -> bool:
        """Check if the current model supports vision/multimodal inputs"""

        # If multimodal is disabled globally, no models support vision
        if not config_provider.get_is_multimodal_enabled():
            return False

        # Original vision detection logic continues...
        config = self.chat_config if config_type == "chat" else self.inference_config
        model_name = config.model.lower()

        logger.info(f"Checking if model '{config.model}' supports vision capabilities")

        # Known vision models - expanded list
        vision_models = [
            # OpenAI models
            "gpt-4-vision",
            "gpt-4v",
            "gpt-4-turbo",
            "gpt-4o",
            "gpt-4o-mini",
            "gpt-4.1",
            "gpt-4.1-mini",
            "o4-mini",
            # Anthropic models
            "claude-3",
            "claude-3-sonnet",
            "claude-3-opus",
            "claude-3-haiku",
            "claude-sonnet-4",
            "claude-opus-4-1",
            "claude-haiku-4-5",
            "claude-sonnet-4-5",
            # Google models
            "gemini-pro-vision",
            "gemini-1.5",
            "gemini-1.5-pro",
            "gemini-1.5-flash",
            "gemini-2.0",
            "gemini-2.0-flash",
            "gemini-2.5",
            "gemini-2.5-pro",
            "gemini-ultra",
            # Other models that might support vision
            "deepseek-chat",
            "llama-3.3",
            "llama-3.3-70b",
            "llama-3.3-8b",
        ]

        is_vision = any(vision_model in model_name for vision_model in vision_models)
        logger.info(f"Model '{config.model}' vision support: {is_vision}")

        if not is_vision:
            logger.warning(
                f"Model '{config.model}' may not support vision. Known vision models: {vision_models}"
            )

        return is_vision

    def get_pydantic_model(
        self, provider: str | None = None, model: str | None = None
    ) -> Model | None:
        """Get the appropriate PydanticAI model based on the active provider."""
        target_model = model or self.chat_config.model
        config = self._build_config_for_model_identifier(target_model)

        if provider:
            config.provider = provider
            config.auth_provider = provider

        api_key = self._get_api_key(config.auth_provider)
        if not api_key and config.auth_provider == "ollama":
            api_key = os.environ.get("OLLAMA_API_KEY", "ollama")
        if not api_key:
            api_key = os.environ.get("LLM_API_KEY", api_key)

        if not api_key and config.auth_provider not in {"ollama"}:
            raise UnsupportedProviderError(
                f"API key not found for provider '{config.auth_provider}'."
            )

        model_name = (
            target_model.split("/", 1)[1] if "/" in target_model else target_model
        )

        if not config.capabilities.get("supports_pydantic", False):
            raise UnsupportedProviderError(
                f"Model '{target_model}' does not support Pydantic-based agents."
            )

        provider_kwargs = {}
        if config.base_url:
            provider_kwargs["base_url"] = config.base_url
        if config.api_version:
            provider_kwargs["api_version"] = config.api_version

        openai_like_providers = {"openai", "openrouter", "azure", "ollama"}
        if config.auth_provider in openai_like_providers:
            if config.auth_provider == "ollama":
                base_url_root = (
                    config.base_url
                    or os.environ.get("LLM_API_BASE")
                    or "http://localhost:11434"
                )
                provider_kwargs["base_url"] = base_url_root.rstrip("/") + "/v1"
            return OpenAIModel(
                model_name=model_name,
                provider=OpenAIProvider(
                    api_key=api_key,
                    **provider_kwargs,
                ),
            )

        if config.provider == "anthropic":
            anthropic_kwargs = {
                key: value
                for key, value in provider_kwargs.items()
                if key != "api_version"
            }
            return AnthropicModel(
                model_name=model_name,
                provider=AnthropicProvider(
                    api_key=api_key,
                    **anthropic_kwargs,
                ),
            )

        raise UnsupportedProviderError(
            f"Provider '{config.provider}' is not supported for Pydantic-based agents."
        )


 from typing import List

from fastapi import Depends, APIRouter
from sqlalchemy.orm import Session

from core.dependencies import get_database_session, get_current_user
from modules.auth.models.user import User

from .provider_controller import ProviderController
from .provider_schema import (
    ProviderInfo,
    SetProviderRequest,
    GetProviderResponse,
    AvailableModelsResponse,
)

router = APIRouter(prefix="/provider", tags=["provider"])


class ProviderAPI:
    @staticmethod
    @router.get("/list-available-llms/", response_model=List[ProviderInfo])
    async def list_available_llms(
        db: Session = Depends(get_database_session),
        user: User = Depends(get_current_user),
    ):
        """List available LLM providers."""
        user_id = user.id
        controller = ProviderController(db, user_id)
        return await controller.list_available_llms()

    @staticmethod
    @router.get("/list-available-models/", response_model=AvailableModelsResponse)
    async def list_available_models(
        db: Session = Depends(get_database_session),
        user: User = Depends(get_current_user),
    ):
        """List available models for both chat and inference."""
        user_id = user.id
        controller = ProviderController(db, user_id)
        return await controller.list_available_models()

    @staticmethod
    @router.post("/set-global-ai-provider/")
    async def set_global_ai_provider(
        provider_request: SetProviderRequest,
        db: Session = Depends(get_database_session),
        user: User = Depends(get_current_user),
    ):
        """Update the global AI provider configuration."""
        user_id = user.id
        controller = ProviderController(db, user_id)
        return await controller.set_global_ai_provider(user_id, provider_request)

    @staticmethod
    @router.get("/get-global-ai-provider/", response_model=GetProviderResponse)
    async def get_global_ai_provider(
        db: Session = Depends(get_database_session),
        user: User = Depends(get_current_user),
    ):
        """Get the current global AI provider configuration."""
        user_id = user.id
        controller = ProviderController(db, user_id)
        return await controller.get_global_ai_provider(user_id)

 from typing import List, Optional
from pydantic import BaseModel


class ProviderInfo(BaseModel):
    id: str
    name: str
    description: str


class AvailableModelOption(BaseModel):
    id: str  # Full model identifier (e.g., "anthropic/claude-3-5-haiku-20241022")
    name: str  # Display name
    description: str
    provider: str  # Provider identifier (e.g., "anthropic")
    is_chat_model: bool  # Whether this is a chat model
    is_inference_model: bool  # Whether this is an inference model


class AvailableModelsResponse(BaseModel):
    models: List[AvailableModelOption]


class SetProviderRequest(BaseModel):
    chat_model: Optional[str] = None  # Full model identifier for chat
    inference_model: Optional[str] = None  # Full model identifier for inference


class ModelInfo(BaseModel):
    provider: str
    id: str
    name: str


class GetProviderResponse(BaseModel):
    chat_model: Optional[ModelInfo] = None
    inference_model: Optional[ModelInfo] = None


class DualProviderConfig(BaseModel):
    chat_config: GetProviderResponse
    inference_config: GetProviderResponse              